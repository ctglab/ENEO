{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>ENEO is a Snakemake workflow developed for the identification of cancer neoantigens using solely the tumor RNAseq, without requiring matched controls or additional sequencing experiments. You could read more from the publication here.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To execute the pipeline, it's required to have both snakemake and singularity or apptainer installed. The easiest way to install both of them is using a dedicated conda environment. To create a new environment, use the following commands:</p> <pre><code>conda create -c conda-forge -c bioconda -c nodefaults -n snakemake snakemake apptainer\n</code></pre> <p>To start, clone the repo using </p> <pre><code>git clone https://github.com/ctglab/ENEO.git\n</code></pre> <p>The next step is to setup resources, as reported in the dedicated section. </p> <p>Then update the configuration file in <code>config/config.yaml</code> following the instructions in the dedicated section.</p> <p>The next step is to setup patients and their relative sequencing files, defined in the files <code>units.csv</code> and <code>patients.csv</code>. The pipeline is designed to be executed both from paired end <code>FASTQ</code> files. Edit then the <code>units.csv</code> file to specify the absolute path for sequencing files of each patient, accordingly:</p> <ul> <li> <p>if you're executing the pipeline in <code>full</code> mode, you had to provide paths to the paired end fastq files, as detailed.</p> <pre><code>patient,fq1,fq2\nPat_01,/path/to/Pat_01_1.fastq.gz,/path/to/Pat_01_2.fastq.gz\nPat_02,/path/to/Pat_02_1.fastq.gz,/path/to/Pat_02_2.fastq.gz\n</code></pre> </li> </ul> <p>Edit also the <code>patients.csv</code> file to add the list of patients to be processed. All the listed patients must match the patients in the <code>units.csv</code> file.</p> <pre><code>```\npatient\nPat_01\nPat_02\n```\n</code></pre>"},{"location":"#executing-the-pipeline","title":"Executing the pipeline","text":"<p>If you're running the pipeline using an HPC cluster, we provided detailed execution profiles for SLURM schedulers under the <code>workflow/profile</code> directory and the relative instruction in the dedicated section.</p> <p>Alternatively, you can run the pipeline locally, using the following command:</p> <pre><code>snakemake \\\n--use-singularity \\\n--cores 8 \\\n--singularity-args \"-B /path/to/ENEO -B /path/to/ENEO_output -B /path/to/ENEO_temp -B /path/to/eneo_resources --env TMPDIR=/path/to/ENEO_temp\"\n</code></pre>"},{"location":"#reporting-issue","title":"Reporting issue","text":"<p>If you encounter any error in the pipeline setup/execution, or you have any question regarding its usage, fill an issue on github </p>"},{"location":"hpc/","title":"Run on HPC","text":"<p>ENEO was developed and tested in High Performances Computing (HPC) clusters with the SLURM workload manager. It's strongly suggested to use a recent version of Snakemake (&gt;8.0.0) that works smootly with the slurm executor plugin. </p> <p>Install the SLURM executor plugin with</p> <pre><code>pip install snakemake-executor-plugin-slurm\n</code></pre> <p>Then inside the folder <code>worflow/profile/slurm</code> you'll find a configuration file named <code>config.yaml</code>, where you should add the details about your SLURM account and desired partition.  </p>"},{"location":"hpc/#singularity-args","title":"Singularity args","text":"<p>Two rules of the workflow (variant annotation and pMHC binding affinity estimation) depend on Singularity containers. It's key to ensure that all the relevant folders are readable/writable within each container. For this reason, multiple folders are required to be mounted, as Snakemake is lazy in assigning mountpoints. </p> <p>Populate the last entry of the config file, <code>singularity-args</code>, adding the absolute path for:</p> <ul> <li>the resources directory</li> <li>the temporary directory</li> <li>the output directory</li> <li>the workflow directory</li> </ul> <p>Additionally, you had to set the TMPDIR environment variable to the temporary directory, to avoid writing permissions in the last step.</p>"},{"location":"hpc/#slurm","title":"SLURM","text":"<p>Insert the account and partition inside <code>workflow/profile/slurm_profile/config.yaml</code> and any other additional flags required for submitting jobs on the HPC platform in use. </p> <pre><code>executor: slurm\ndefault-resources:\n    slurm_account: \n    slurm_partition: \n    mem_mb_per_cpu: 8000\n    runtime: \"60m\"\n\nuse-apptainer: true\nkeep-going: true\nrerun-incomplete: true\njobs: 100\nsingularity-args: '-B /../ENEO_res -B /../ENEO_temp -B /../ENEO_output -B /../ENEO/workflow --env TMPDIR=/../ENEO_temp'\n</code></pre> <p>For additional guidelines on how to compile the profile and how to execute it under the SLURM scheduler, refer to the executor plugin documentation </p> <pre><code>snakemake --profile workflow/profile/slurm_profile\n</code></pre>"},{"location":"intervals/","title":"Calling intervals","text":"<p>By default, ENEO performs variant calling on exons of protein coding genes, with the exception of two types of calling regions:</p> <ul> <li> <p>Known hard-to-call regions of the human genome. These regions were extensively profiled by the GIAB consortium, and are publicly available. This regions are removed from the VCF file and so are not present in the final VCF output.</p> </li> <li> <p>Genes involved in the antigen presentation: given the initial goal of the pipeline to be applied in a personalized cancer vaccine setup, we discared these genes as a source of unwanted variations. We obtained the set of genes using the KEGG pathway annotated as </p> </li> </ul> Expand to see the full set of excluded genes in ENEO symbol description IFNG interferon gamma TNF tumor necrosis factor PSME1 proteasome activator subunit 1 PSME2 proteasome activator subunit 2 PSME3 proteasome activator subunit 3 HSPA8 heat shock protein family A (Hsp70) member 8 HSPA1A heat shock protein family A (Hsp70) member 1A HSPA1L heat shock protein family A (Hsp70) member 1 like HSPA1B heat shock protein family A (Hsp70) member 1B HSPA6 heat shock protein family A (Hsp70) member 6 HSPA2 heat shock protein family A (Hsp70) member 2 HSPA4 heat shock protein family A (Hsp70) member 4 HSP90AA1 heat shock protein 90 alpha family class A member 1 HSP90AB1 heat shock protein 90 alpha family class B member 1 HLA-A major histocompatibility complex, class I, A HLA-B major histocompatibility complex, class I, B HLA-C major histocompatibility complex, class I, C HLA-F major histocompatibility complex, class I, F HLA-G major histocompatibility complex, class I, G HLA-E major histocompatibility complex, class I, E HSPA5 heat shock protein family A (Hsp70) member 5 CANX calnexin B2M beta-2-microglobulin PDIA3 protein disulfide isomerase family A member 3 CALR calreticulin TAPBP TAP binding protein TAP1 transporter 1, ATP binding cassette subfamily B member TAP2 transporter 2, ATP binding cassette subfamily B member CD8A CD8 subunit alpha CD8B CD8 subunit beta CD8B2 CD8B family member 2 KIR3DL2 killer cell immunoglobulin like receptor, three Ig domains and long cytoplasmic tail 2 KIR3DL1 killer cell immunoglobulin like receptor, three Ig domains and long cytoplasmic tail 1 KIR3DL3 killer cell immunoglobulin like receptor, three Ig domains and long cytoplasmic tail 3 KIR3DS1 killer cell immunoglobulin like receptor, three Ig domains and short cytoplasmic tail 1 KIR2DL2 killer cell immunoglobulin like receptor, two Ig domains and long cytoplasmic tail 2 KIR2DL1 killer cell immunoglobulin like receptor, two Ig domains and long cytoplasmic tail 1 KIR2DL3 killer cell immunoglobulin like receptor, two Ig domains and long cytoplasmic tail 3 KIR2DL4 killer cell immunoglobulin like receptor, two Ig domains and long cytoplasmic tail 4 KIR2DL5A killer cell immunoglobulin like receptor, two Ig domains and long cytoplasmic tail 5A KLRC1 killer cell lectin like receptor C1 KLRC2 killer cell lectin like receptor C2 KLRC3 killer cell lectin like receptor C3 KLRC4 killer cell lectin like receptor C4 KLRD1 killer cell lectin like receptor D1 KIR2DS1 killer cell immunoglobulin like receptor, two Ig domains and short cytoplasmic tail 1 KIR2DS3 killer cell immunoglobulin like receptor, two Ig domains and short cytoplasmic tail 3 KIR2DS4 killer cell immunoglobulin like receptor, two Ig domains and short cytoplasmic tail 4 KIR2DS5 killer cell immunoglobulin like receptor, two Ig domains and short cytoplasmic tail 5 KIR2DS2 killer cell immunoglobulin like receptor, two Ig domains and short cytoplasmic tail 2 IFI30 IFI30 lysosomal thiol reductase LGMN legumain CTSB cathepsin B HLA-DMA major histocompatibility complex, class II, DM alpha HLA-DMB major histocompatibility complex, class II, DM beta HLA-DOA major histocompatibility complex, class II, DO alpha HLA-DOB major histocompatibility complex, class II, DO beta HLA-DPA1 major histocompatibility complex, class II, DP alpha 1 HLA-DPB1 major histocompatibility complex, class II, DP beta 1 HLA-DQA1 major histocompatibility complex, class II, DQ alpha 1 HLA-DQA2 major histocompatibility complex, class II, DQ alpha 2 HLA-DQB1 major histocompatibility complex, class II, DQ beta 1 HLA-DRA major histocompatibility complex, class II, DR alpha HLA-DRB1 major histocompatibility complex, class II, DR beta 1 HLA-DRB3 major histocompatibility complex, class II, DR beta 3 HLA-DRB4 major histocompatibility complex, class II, DR beta 4 HLA-DRB5 major histocompatibility complex, class II, DR beta 5 CD74 CD74 molecule CTSL cathepsin L CTSS cathepsin S CD4 CD4 molecule CIITA class II major histocompatibility complex transactivator RFX5 regulatory factor X5 RFXANK regulatory factor X associated ankyrin containing protein RFXAP regulatory factor X associated protein CREB1 cAMP responsive element binding protein 1 NFYA nuclear transcription factor Y subunit alpha NFYB nuclear transcription factor Y subunit beta NFYC nuclear transcription factor Y subunit gamma"},{"location":"intervals/#generate-a-custom-interval-set","title":"Generate a custom interval set","text":"<p>To build a custom set of intervals from a GTF file (here referred to as <code>genecode.gtf.gz</code>), you need to have <code>tabix</code> and <code>bgzip</code> installed. If you built a conda environment for the setup using the <code>setup_env.yml</code> file, <code>tabix</code> and <code>bgzip</code> will be present.</p> <pre><code>zgrep \"protein_coding\" gencode.gtf.gz | awk -F'\\t' '{ if ($3 == \"exon\") print $1, $2, $3}' OFS='\\t' - | bgzip -c &gt; calling_intervals.BED.gz &amp;&amp;\\\ntabix -p bed calling_intervals.BED.gz\n</code></pre> <p>Then you can use the <code>calling_intervals.BED.gz</code> file as the input for the variant calling step, by putting it in the <code>config_main.yaml</code></p>"},{"location":"resources/","title":"Setup resources","text":"<p>ENEO heavily depends on public genetic databases for germline probability estimation plus other fairly common resources daily needed for bioinformatics. In order to make the download and configuration of resources for the workflow as smooth as possible, a python configuration script is available inside <code>setup/download_res.py</code>. As the workflow uses files with different annotations, downloading resources is not enough, as they may need a chromosome naming conversion: thus some common used tools like <code>bcftools</code> are required. To run the automatic setup, first create a <code>conda</code> environment using the <code>setup_env.yml</code> file located inside the <code>setup</code> folder.</p> <pre><code>conda env create -f setup_env.yml \n</code></pre> <p>Then activate the conda environment, and launch the configuration script as follows, replacing the <code>path_for_resources</code> folder with a PATH with at least 60GB of free space and where you have full read/write access.</p> <pre><code>python3 download_res.py -o &lt;path_for_resources&gt;  \n</code></pre> <p>Note</p> <p>The genome assembly in use is the GRCh38. We're not planning to backport it to older assembly. </p> <p>This script will download different resources:</p> <ul> <li>genome from GIAB, the <code>GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18</code> version. Why? Look here. </li> <li>transcriptome and GTF from Gencode</li> <li>1000G, ExAC, 1000g PoN from the GATK resource bundle</li> <li>dbSNPs ALFA from NCBI</li> <li>REDI portal</li> <li>VEP cache (v105)</li> </ul>"},{"location":"resources/#what-if-i-already-got-some-of-them","title":"What if I already got some of them?","text":"<p>The configuration script works by controlling the existence of the files whose path is written inside the main configuration file <code>conf_main.yaml</code>, located in the <code>config</code> folder. If any of those files are already in your machine, just edit the configuration file adding the right absolute path. The script will check for its presence without re-downloading it.</p> <p>Failure</p> <p>The annotation for files must be concordant throughout the pipeline, to avoid any error. The chromosome notation in use is the one by Gencode, thus with <code>chr</code>. Be sure to add resources manually only if you're sure of their concordance.</p>"},{"location":"setup/","title":"Workflow Setup","text":"<p>The pipeline uses the configuration file hosted in the <code>config</code> directory, named <code>config_main.yaml</code>. It's a YAML file divided into sections, that is actively read by Snakemake to obtain informations about resources, parameters and paths. While we tried to make it as user-friendly as possible, enabling the automatic update during the downloading of the resources (refer to the Setup resources section), some tweaks are required to do manually.</p> <p>The configuration file is divided into sections, and here we provide a detailed explanation of each section and its content.</p>"},{"location":"setup/#general","title":"General","text":"<p>The first three parameters are put on top of the configuration file, and they are the most important ones. They are:</p> <ul> <li><code>execution_mode</code>: it's just a placeholder. Leave it with <code>full</code>.</li> <li><code>OUTPUT_FOLDER</code>: the path to the output folder where the results will be saved. It is mandatory to provide the absolute path.</li> <li><code>TEMP_DIR</code>: the path to the temporary folder where the intermediate files will be saved. It is mandatory to provide the absolute path.</li> </ul> <pre><code>execution_mode: \"full\"\nOUTPUT_FOLDER: /path/to/../ENEO_output/\nTEMP_DIR: /path/to/../ENEO_temp/\n</code></pre> <p>Note</p> <p>Both the <code>OUTPUT_FOLDER</code> and <code>TEMP_DIR</code> paths must be absolute, and they're placed on top as they must be provided as mounting points in the <code>SINGULARITY_ARGS</code> parameters Snakemake.</p>"},{"location":"setup/#datadirs","title":"Datadirs","text":"<p>Each entry in this section defines the name of the output folders that will be created in the <code>OUTPUT_FOLDER</code> path. The name of the folder is the key, and the value is the path to the folder. The pipeline will create the folders if they don't exist, and will save the results in the corresponding folder.  We suggest to keep the default values , but you can change them if you want.</p> Folder Name Description <code>bams</code> the name of the folder used during .BAM files processing. <code>BQSR</code> the name of the folder where the recalibrated .BAM files will be saved. <code>HLA_typing</code> the name of the folder where the HLA typing results will be saved. <code>index_folder</code> the name of the folder where the STAR index will be saved. <code>mapped_reads</code> the name of the folder where the results of STAR will be saved. <code>peptides</code> the name of the folder where the pMHC prediction will be saved. <code>utils</code> the name of the folder used for workflow utilities. <code>VCF</code> the name of the folder where the RAW .VCF files will be saved. <code>VCF_out</code> the name of the folder where the processed .VCF files will be saved."},{"location":"setup/#parameters","title":"Parameters","text":"<p>This section contains the parameters used by the various steps of the pipeline. The majority of the parameters are standard and should not be changed. We suggest anyway users to focus on the <code>pMHC</code> section of the parameters, which reports</p> <pre><code>  pMHC:\n    threads: 4\n    netmhcpan_launcher_script: workflow/scripts/netmhcpan_launcher.py\n    calibration_frame: workflow/supplementary_res/optimal_percentile_netmhcpan.csv\n    hla_ligand_atlas: workflow/supplementary_res/HLA_ligand_atlas.tsv.gz\n    filter_peptides_script: workflow/scripts/filter_peptides.py\n    min_length: 8\n    max_length: 12\n    germProb: 0.5\n</code></pre> <p>The last three parameters defines the constrains used for the mutated peptide generation and the subsequent </p> Parameter Name Description <code>min_length</code> Minimum length of the peptide. <code>max_length</code> Maximum length of the peptide. <code>germProb</code> Threshold for the germline probability of the variant generating the mutated peptide."},{"location":"setup/#resources","title":"Resources","text":"<p>This section contains the paths to the resources used by the pipeline. Most of the resources could be downloaded automatically using the script provided in the folder <code>setup</code> named <code>download_res.py</code>. The path for the downloaded files will be updated automatically (refer to the Setup resources section).</p> <p>Note</p> <p>All the paths must be absolute. Note that it's generally preferred to keep all the resources in the same folder, but this is not mandatory. The reason is due to the fact that these files must be accessible by the Singularity container, and the paths must be provided as mounting points in the <code>SINGULARITY_ARGS</code> parameter when executing the pipeline. If all the files are in the same folder, you can provide the path to the folder, and the pipeline will be able to access all the files.</p>"}]}